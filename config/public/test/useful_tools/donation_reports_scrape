#!/usr/bin/env python

# recursively scrapes http://seeyourimpact.org/database/donation_reports.php

import optparse
import re
import time
import tempfile
import syi
import urllib2
import codecs

# return a query string for the next url to scrape
def get_next_link(html):
    m = re.search(r'\?(.*?)">NEXT PAGE', html)
    if m:
        if opts.bydonor:
            raise Exception('no NEXT PAGE links are allowed with --bydonor')
        else:
            return m.group(1)
    else:
        if opts.bydonor:
            return next_donor_ids()
        else:
            return ''

def html_filter(html):
    html = re.sub(r'(<html><head>)', r'\1<meta http-equiv="Content-Type" content="text/html; charset=utf-8">', html)
    # html output
    try:
        opts.html.write(re.sub(r'<tr', "\n<tr", html))
    except:
        tmp = tempfile.NamedTemporaryFile(delete=False)
        f = codecs.open(tmp.name, 'w', 'utf8')
        f.write(html)
        print 'error writing to html filehandle: %s' % (tmp.name)
        raise

def scrape(url):
    this_url = url
    http_errs = 5

    if opts.query_string:
        this_url += '?' + opts.query_string

    print 'url: ' + this_url

    req = urllib2.Request(this_url)
    if opts.cookies:
        req.add_header('Cookie', opts.cookies)

    while True:
        try:
            html = urllib2.urlopen(req).read().decode('utf8')
        except:
            if http_errs == 0:
                raise Exception('too many failures attempting to fetch %s' % (this_url))
            print '%d attempts remaining to fetch %s' % (http_errs, this_url)
            http_errs -= 1
        else:
            break

    if opts.scrape_count == 0:
        next = get_next_link(html)
        if next:
            m = re.search(r'^(.*?)</table>.*', html, re.DOTALL)
            html_filter(m.group(1))
            opts.scrape_count += 1
            opts.query_string = next;
            scrape(url)
            opts.html.write('</table>')

        else:
            html_filter(html)
    else:
        html_filter(re.search(r'</th></tr>(.*?)</table>', html, re.DOTALL).group(1))
        next = get_next_link(html)
        if next:
            opts.scrape_count += 1
            opts.query_string = next
            scrape(url)

def main():
    t0 = time.time()

    # parse the command line
    usage = """%prog [options]

 example:
     %prog -q 'from=2012-12-01&to=2012-12-31' -o /tmp/dec_2012 -c cookiedump.txt"""

    cmdparser = optparse.OptionParser(usage,version="donation_reports_scrape v0.1")
    cmdparser.add_option("--query-string", '-q',
        help="use this as the url query string")
    cmdparser.add_option("--domain", '-d', default="live",
        help="specify abbreviated domain host, eg 'dev3' (default:'live')")
    cmdparser.add_option("--cookies", '-c', metavar='FILENAME',
        help="filename of cookie dump")
    cmdparser.add_option("--outfile", '-o', metavar='BASE_FILENAME',
        help="specify the filename prefix to write html to (default: random tempfile)")
    cmdparser.add_option("--infile", '-i', metavar='FILENAME',
        help="specify an html file to use as input, rather than fetching html from web")
    cmdparser.add_option("--bydonor", '-b', action="store_true",
        help="iterate through donor IDs")
    cmdparser.add_option("--include-errors", '-e', action="store_true",
        help="include ERROR rows in sql output")

    global opts
    (opts, args) = cmdparser.parse_args()

    if args:
        raise Exception('unknown options')

    if opts.infile:
        # easy
        syi.html_to_fields(opts.infile, 'donation_report', sql_fixup)
    else:
        if opts.domain == 'live':
            url = 'http://seeyourimpact.org/database/donation_reports.php'
        else:
            url = 'http://' + opts.domain + '.seeyourimpact.com/database/donation_reports.php'

        if opts.cookies:
            opts.cookies = syi.cookieFromFile(opts.cookies, url)
            if opts.cookies == '':
                raise Exception("wasn't able to read cookies from cookie file")

        if not opts.outfile:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            opts.outfile = tmp.name

        if opts.bydonor:
            setup_donor_id_buckets()
            opts.query_string = next_donor_ids()

        opts.html = codecs.open(opts.outfile + '.html', 'w', 'utf8')
        print "html output is in " + opts.outfile + '.html'

        opts.scrape_count = 0
        scrape(url)
        elapsed(t0)
        opts.html.close()

        syi.html_to_fields(opts.outfile + '.html', 'donation_report', sql_fixup)

    elapsed(t0)

def elapsed(t):
    e = int(time.time() - t)
    mins = e / 60
    secs = e - mins * 60
    print "elapsed: %02d:%02d" % (mins, secs)

# syi.html_to_fields() uses this function translate each row (passed in as a
# list of columns) to sql-compatible values
def sql_fixup(cols):
    if len(cols) != 17:
        if opts.include_errors and len(cols) == 16 and cols[12] == 'ERROR':
            cols.append('')
        else:
            return []

    if re.search(r'BALANCE', cols[12]):
        return []

    for i in [2,5,6,15]:
        cols[i] = re.sub(r'#', '', cols[i])
    for i in [7,8,9,10,11]:
        cols[i] = re.sub(r'\$', '', cols[i])
    cols[4] = re.sub(r'^(\d+)/(\d+)/(\d+)', r'20\3-\1-\2', cols[4])

    def addcol(m):
        if m.groups():
            cols.append(m.group(1))
        return ''

    cols[16] = re.sub(r'\s*DAT\s*#\s*(\d+)\s*', addcol, cols[16])
    cols[16] = re.sub(r'\s*ACCT\s*#\s*(\d+)\s*', addcol, cols[16])
    cols[16] = re.sub(r'\s*PMT\s*#\s*\d+\s*', '', cols[16]);

    while len(cols) != 19:
        cols.append('')

    return cols

# gets all user IDs from wp_users, and breaks them into lists of 100
# ids at a time
def setup_donor_id_buckets():
    dbh = syi.db(opts.domain)
    c = dbh.cursor()
    c.execute("select id from donationGiver order by id asc")

    opts.bydonor = []
    bucket = []
    count = 0

    while True:
        row = c.fetchone()
        if not row:
            break

        bucket.append(row[0])
        count += 1
        if len(bucket) >= 200:
            opts.bydonor.append(bucket)
            bucket = []

    if len(bucket) > 0:
        opts.bydonor.append(bucket)

    print "total donor ids: %d, number of GETs: %d" % (count, len(opts.bydonor))

# returns opts.query_string with "donor" param set to next bucket of donor IDs
def next_donor_ids():
    qs = opts.query_string
    if not qs:
        qs = ''

    if len(opts.bydonor) == 0:
        return qs

    qs = re.sub(r'donor=[\d,]+', r'', qs)
    if not re.search(r'&$', qs):
        qs += '&'

    qs += 'donor=' + ','.join([str(x) for x in opts.bydonor.pop(0)])
    return qs

if __name__ == '__main__' : main()
